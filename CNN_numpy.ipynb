{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def zero_pad(X, pad):\n",
    "    \n",
    "    X_pad =  np.pad(X, ((0,0), (pad,pad), (pad,pad),(0,0) ),'constant', constant_values=0 )\n",
    "    return X_pad\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape = (4, 3, 3, 2)\n",
      "x_pad.shape = (4, 9, 9, 2)\n",
      "x[1, 1] = [[ 0.90085595 -0.68372786]\n",
      " [-0.12289023 -0.93576943]\n",
      " [-0.26788808  0.53035547]]\n",
      "x_pad[1, 1] = [[ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f4010df7b70>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUgAAACuCAYAAABUfpQYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f40112393c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "np.random.seed(1)\n",
    "x = np.random.randn(4, 3, 3, 2)\n",
    "x_pad = zero_pad(x, 3)\n",
    "print (\"x.shape =\", x.shape)\n",
    "print (\"x_pad.shape =\", x_pad.shape)\n",
    "print (\"x[1, 1] =\", x[1, 1])\n",
    "print (\"x_pad[1, 1] =\", x_pad[1, 1])\n",
    "\n",
    "fig, axarr = plt.subplots(1, 2)\n",
    "axarr[0].set_title('x')\n",
    "axarr[0].imshow(x[0,:,:,0])\n",
    "axarr[1].set_title('x_pad')\n",
    "axarr[1].imshow(x_pad[0,:,:,0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: conv_single_step\n",
    "\n",
    "def conv_single_step(a_slice_prev, W, b):\n",
    "    \"\"\"\n",
    "    Apply one filter defined by parameters W on a single slice (a_slice_prev) of the output activation \n",
    "    of the previous layer.\n",
    "    \n",
    "    Arguments:\n",
    "    a_slice_prev -- slice of input data of shape (f, f, n_C_prev)\n",
    "    W -- Weight parameters contained in a window - matrix of shape (f, f, n_C_prev)\n",
    "    b -- Bias parameters contained in a window - matrix of shape (1, 1, 1)\n",
    "    \n",
    "    Returns:\n",
    "    Z -- a scalar value, result of convolving the sliding window (W, b) on a slice x of the input data\n",
    "    \"\"\"\n",
    "    Z  =  np.multiply(a_slice_prev, W) + b  # element wise product\n",
    "    Z = np.sum(Z) # sum of all the entries of the volume S\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = -23.1602122025\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "a_slice_prev = np.random.randn(4, 4, 3)\n",
    "W = np.random.randn(4, 4, 3)\n",
    "b = np.random.randn(1, 1, 1)\n",
    "\n",
    "Z = conv_single_step(a_slice_prev, W, b)\n",
    "print(\"Z =\", Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# GRADED FUNCTION: conv_forward\n",
    "\n",
    "def conv_forward(A_prev, W, b, hparameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for a convolution function\n",
    "    \n",
    "    Arguments:\n",
    "    A_prev -- output activations of the previous layer, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    W -- Weights, numpy array of shape (f, f, n_C_prev, n_C)\n",
    "    b -- Biases, numpy array of shape (1, 1, 1, n_C)\n",
    "    hparameters -- python dictionary containing \"stride\" and \"pad\"\n",
    "        \n",
    "    Returns:\n",
    "    Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache of values needed for the conv_backward() function\n",
    "    \"\"\"\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    # Retrieve information from \"hparameters\" (≈2 lines)\n",
    "    stride = hparameters['stride']\n",
    "    pad = hparameters['pad']\n",
    "    \n",
    "    n_H = int((n_H_prev - f + 2* pad )/stride ) + 1\n",
    "    n_W = int((n_W_prev - f + 2* pad )/stride ) + 1\n",
    "    \n",
    "    Z = np.zeros((m, n_H, n_W, n_C))\n",
    "    \n",
    "    A_prev_pad =  zero_pad(A_prev, pad )\n",
    "    #Z  =  np.multiply(A_prev, W) + b\n",
    "    \n",
    "    for i in range(m):\n",
    "        a_prev_pad = A_prev_pad[i]\n",
    "        \n",
    "        for h in range(n_H):\n",
    "            for w in range(n_W):\n",
    "                for c in range(n_C):\n",
    "                    vert_start = h * stride # height\n",
    "                    vert_end = vert_start + f \n",
    "                    horiz_start = w * stride # width\n",
    "                    horiz_end = horiz_start+ f \n",
    "                    \n",
    "                    \n",
    "                    a_slice_prev =  a_prev_pad[vert_start:vert_end, horiz_start:horiz_end , :]\n",
    "                    \n",
    "                    Z[i, h, w, c] =  conv_single_step(a_slice_prev, W[...,c], b[...,c])\n",
    "                    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(Z.shape == (m, n_H, n_W, n_C))\n",
    "    \n",
    "    # Save information in \"cache\" for the backprop\n",
    "    cache = (A_prev, W, b, hparameters)\n",
    "    \n",
    "    return Z, cache       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z's mean = 0.155859324889\n",
      "cache_conv[0][1][2][3] = [-0.20075807  0.18656139  0.41005165]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(10, 4, 4, 3)\n",
    "W = np.random.randn(2, 2, 3, 8)\n",
    "b = np.random.randn(1, 1, 1, 8)\n",
    "hparameters = {\"pad\" : 2,\n",
    "               \"stride\": 1}\n",
    "\n",
    "Z, cache_conv = conv_forward(A_prev, W, b, hparameters)\n",
    "print(\"Z's mean =\", np.mean(Z))\n",
    "print(\"cache_conv[0][1][2][3] =\", cache_conv[0][1][2][3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pool_forward(A_prev, hparameters, mode = \"max\"):\n",
    "    \"\"\"\n",
    "    Implements the forward pass of the pooling layer\n",
    "    \n",
    "    Arguments:\n",
    "    A_prev -- Input data, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    hparameters -- python dictionary containing \"f\" and \"stride\"\n",
    "    mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of the pool layer, a numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache used in the backward pass of the pooling layer, contains the input and hparameters \n",
    "    \"\"\"\n",
    "    \n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve hyperparameters from \"hparameters\"\n",
    "    f = hparameters[\"f\"]\n",
    "    stride = hparameters[\"stride\"]\n",
    "    \n",
    "    # Define the dimensions of the output\n",
    "    n_H = int(1 + (n_H_prev - f) / stride)\n",
    "    n_W = int(1 + (n_W_prev - f) / stride)\n",
    "    n_C = n_C_prev\n",
    "    \n",
    "    # Initialize output matrix A\n",
    "    A = np.zeros((m, n_H, n_W, n_C)) \n",
    "    \n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    for i in range(m):                           # loop over the training examples\n",
    "        for h in range(n_H):                     # loop on the vertical axis of the output volume\n",
    "            for w in range(n_W):                 # loop on the horizontal axis of the output volume\n",
    "                for c in range (n_C):            # loop over the channels of the output volume\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\" (≈4 lines)\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    # Use the corners to define the current slice on the ith training example of A_prev, channel c. (≈1 line)\n",
    "                    a_prev_slice = A_prev[i, vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "                    \n",
    "                    # Compute the pooling operation on the slice. Use an if statment to differentiate the modes. Use np.max/np.mean.\n",
    "                    if mode == \"max\":\n",
    "                        A[i, h, w, c] = np.max(a_prev_slice)\n",
    "                    elif mode == \"average\":\n",
    "                        A[i, h, w, c] = np.mean(a_prev_slice)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Store the input and hparameters in \"cache\" for pool_backward()\n",
    "    cache = (A_prev, hparameters)\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(A.shape == (m, n_H, n_W, n_C))\n",
    "    \n",
    "    return A, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode = max\n",
      "A = [[[[ 1.74481176  1.6924546   2.10025514]]]\n",
      "\n",
      "\n",
      " [[[ 1.19891788  1.51981682  2.18557541]]]]\n",
      "\n",
      "mode = average\n",
      "A = [[[[-0.09498456  0.11180064 -0.14263511]]]\n",
      "\n",
      "\n",
      " [[[-0.09525108  0.28325018  0.33035185]]]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(2, 4, 4, 3)\n",
    "hparameters = {\"stride\" : 1, \"f\": 4}\n",
    "\n",
    "A, cache = pool_forward(A_prev, hparameters)\n",
    "print(\"mode = max\")\n",
    "print(\"A =\", A)\n",
    "print()\n",
    "A, cache = pool_forward(A_prev, hparameters, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print(\"A =\", A)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a convolution function\n",
    "    \n",
    "    Arguments:\n",
    "    dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache of values needed for the conv_backward(), output of conv_forward()\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),\n",
    "               numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    dW -- gradient of the cost with respect to the weights of the conv layer (W)\n",
    "          numpy array of shape (f, f, n_C_prev, n_C)\n",
    "    db -- gradient of the cost with respect to the biases of the conv layer (b)\n",
    "          numpy array of shape (1, 1, 1, n_C)\n",
    "    \"\"\"\n",
    "      \n",
    "    ### START CODE HERE ###\n",
    "    # Retrieve information from \"cache\"\n",
    "    (A_prev, W, b, hparameters) = cache\n",
    "    # Retrieve dimensions from A_prev's shape\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve dimensions from W's shape\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "     # Retrieve information from \"hparameters\"\n",
    "    stride = hparameters[\"stride\"]\n",
    "    pad = hparameters[\"pad\"]\n",
    "    \n",
    "    # Retrieve dimensions from dZ's shape\n",
    "    (m, n_H, n_W, n_C) = dZ.shape\n",
    "    \n",
    "    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))                           \n",
    "    dW = np.zeros((f, f, n_C_prev, n_C))\n",
    "    db = np.zeros((1, 1, 1, n_C))\n",
    "    \n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    dA_prev_pad = zero_pad(dA_prev, pad)\n",
    "                           \n",
    "    for i in range(m):                       # loop over the training examples\n",
    "        # select ith training example from A_prev_pad and dA_prev_pad\n",
    "        a_prev_pad = A_prev_pad[i]\n",
    "        da_prev_pad = dA_prev_pad[i]\n",
    "        \n",
    "        for h in range(n_H):                   # loop over vertical axis of the output volume\n",
    "            for w in range(n_W):               # loop over horizontal axis of the output volume\n",
    "                for c in range(n_C):           # loop over the channels of the output volume\n",
    "                    vert_start = h\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    a_slice = a_prev_pad[vert_start:vert_end,horiz_start:horiz_end, :]\n",
    "\n",
    "                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n",
    "                    dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
    "                    db[:,:,:,c] += dZ[i, h, w, c]\n",
    "        dA_prev[i, :, :, :] = da_prev_pad[pad:-pad, pad:-pad, :]\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_mean = 9.60899067587\n",
      "dW_mean = 10.5817412755\n",
      "db_mean = 76.3710691956\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "dA, dW, db = conv_backward(Z, cache_conv)\n",
    "print(\"dA_mean =\", np.mean(dA))\n",
    "print(\"dW_mean =\", np.mean(dW))\n",
    "print(\"db_mean =\", np.mean(db))\n",
    "# print(dA.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_mask_from_window(x):\n",
    "    \"\"\"\n",
    "    Creates a mask from an input matrix x, to identify the max entry of x.\n",
    "    \n",
    "    Arguments:\n",
    "    x -- Array of shape (f, f)\n",
    "    \n",
    "    Returns:\n",
    "    mask -- Array of the same shape as window, contains a True at the position corresponding to the max entry of x.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈1 line)\n",
    "    mask = x == np.max(x)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distribute_value(dz, shape):\n",
    "    \"\"\"\n",
    "    Distributes the input value in the matrix of dimension shape\n",
    "    \n",
    "    Arguments:\n",
    "    dz -- input scalar\n",
    "    shape -- the shape (n_H, n_W) of the output matrix for which we want to distribute the value of dz\n",
    "    \n",
    "    Returns:\n",
    "    a -- Array of size (n_H, n_W) for which we distributed the value of dz\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Retrieve dimensions from shape (≈1 line)\n",
    "    (n_H, n_W) = shape\n",
    "    \n",
    "    # Compute the value to distribute on the matrix (≈1 line)\n",
    "    average = dz / (n_H * n_W)\n",
    "    \n",
    "    # Create a matrix where every entry is the \"average\" value (≈1 line)\n",
    "    a = np.ones(shape) * average\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pool_backward(dA, cache, mode = \"max\"):\n",
    "    \"\"\"\n",
    "    Implements the backward pass of the pooling layer\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- gradient of cost with respect to the output of the pooling layer, same shape as A\n",
    "    cache -- cache output from the forward pass of the pooling layer, contains the layer's input and hparameters \n",
    "    mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- gradient of cost with respect to the input of the pooling layer, same shape as A_prev\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Retrieve information from cache (≈1 line)\n",
    "    (A_prev, hparameters) = cache\n",
    "    \n",
    "    # Retrieve hyperparameters from \"hparameters\" (≈2 lines)\n",
    "    stride = hparameters[\"stride\"]\n",
    "    f = hparameters[\"f\"]\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape and dA's shape (≈2 lines)\n",
    "    m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
    "    m, n_H, n_W, n_C = dA.shape\n",
    "    \n",
    "    # Initialize dA_prev with zeros (≈1 line)\n",
    "    dA_prev = np.zeros(A_prev.shape)\n",
    "    \n",
    "    for i in range(m):                       # loop over the training examples\n",
    "        # select training example from A_prev (≈1 line)\n",
    "        a_prev = A_prev[i]\n",
    "        for h in range(n_H):                   # loop on the vertical axis\n",
    "            for w in range(n_W):               # loop on the horizontal axis\n",
    "                for c in range(n_C):           # loop over the channels (depth)\n",
    "                    # Find the corners of the current \"slice\" (≈4 lines)\n",
    "                    vert_start = h\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    # Compute the backward propagation in both modes.\n",
    "                    if mode == \"max\":\n",
    "                        # Use the corners and \"c\" to define the current slice from a_prev (≈1 line)\n",
    "                        a_prev_slice = a_prev[vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "                        # Create the mask from a_prev_slice (≈1 line)\n",
    "                        mask = create_mask_from_window(a_prev_slice)\n",
    "                        # Set dA_prev to be dA_prev + (the mask multiplied by the correct entry of dA) (≈1 line)\n",
    "                        dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, c] += np.multiply(mask, dA[i, h, w, c])\n",
    "                        \n",
    "                    elif mode == \"average\":\n",
    "                        # Get the value a from dA (≈1 line)\n",
    "                        da = dA[i, h, w, c]\n",
    "                        # Define the shape of the filter as fxf (≈1 line)\n",
    "                        shape = (f, f)\n",
    "                        # Distribute it to get the correct slice of dA_prev. i.e. Add the distributed value of da. (≈1 line)\n",
    "                        dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, c] += distribute_value(da, shape)\n",
    "                        \n",
    "    ### END CODE ###\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "    \n",
    "    return dA_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode = max\n",
      "mean of dA =  0.145713902729\n",
      "dA_prev[1,1] =  [[ 0.          0.        ]\n",
      " [ 5.05844394 -1.68282702]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "mode = average\n",
      "mean of dA =  0.145713902729\n",
      "dA_prev[1,1] =  [[ 0.08485462  0.2787552 ]\n",
      " [ 1.26461098 -0.25749373]\n",
      " [ 1.17975636 -0.53624893]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(5, 5, 3, 2)\n",
    "hparameters = {\"stride\" : 1, \"f\": 2}\n",
    "A, cache = pool_forward(A_prev, hparameters)\n",
    "dA = np.random.randn(5, 4, 2, 2)\n",
    "\n",
    "dA_prev = pool_backward(dA, cache, mode = \"max\")\n",
    "print(\"mode = max\")\n",
    "print('mean of dA = ', np.mean(dA))\n",
    "print('dA_prev[1,1] = ', dA_prev[1,1])  \n",
    "print()\n",
    "dA_prev = pool_backward(dA, cache, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print('mean of dA = ', np.mean(dA))\n",
    "print('dA_prev[1,1] = ', dA_prev[1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
